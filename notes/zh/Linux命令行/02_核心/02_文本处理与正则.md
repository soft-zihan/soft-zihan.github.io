# 🧩 文本处理与正则 (The Power of Text)

这是 Linux 区别于 Windows 的最大威力所在：**通过命令行批量处理文本内容**。

## 1. 正则表达式入门 (Regular Expressions)
正则就是“通配符”的升级版，用于精准匹配字符串。你不需要背下所有规则，掌握这 5 个符号就能解决 90% 的问题。

| 符号 | 含义 | 举例 | 匹配结果 |
| :--- | :--- | :--- | :--- |
| **`^`** | **行首** | `^Error` | 匹配以 "Error" **开头**的行 |
| **`$`** | **行尾** | `OK$` | 匹配以 "OK" **结尾**的行 |
| **`.`** | **任意字符** | `a.c` | 匹配 "abc", "a@c", "a1c" |
| **`*`** | **重复** (0次或多次) | `ab*c` | 匹配 "ac", "abc", "abbbc" |
| **`[]`** | **范围** | `[0-9]` | 匹配任意一个数字 |

> ⚠️ **新手陷阱：正则 `*` vs 通配符 `*`**
> - **Shell 通配符**：`ls *.txt` 中的 `*` 代表“任意个字符”。
> - **正则表达式**：`grep "a*"` 中的 `*` 代表“前面的那个字符重复 0 次或多次”。
> - **后果**：在正则里单独打一个 `*` 是无效的，必须配合前面的字符使用（如 `.*` 才等同于 Shell 的 `*`）。

> **实战组合**：
> - `^$`：匹配空行。
> - `^#`：匹配注释行 (以 # 开头)。

---

## 2. grep - 文本搜索 (配合正则)
`grep` 是正则的最佳演练场。

- **基础用法**：
  - `grep "login" app.log`：找出包含 login 的行。
  - **`grep -i "error" app.log`**：忽略大小写。
  - **`grep -v "debug" app.log`**：**反向**查找 (不包含 debug 的行)。
  - **`grep -l "pattern" *.log`**：只列出包含匹配内容的**文件名**。
  - **`grep -c "pattern" file`**：统计匹配到的**行数**。
  - **`grep "\." file`**：**转义特殊字符**。由于 `.` 在正则中代表任意字符，如果你想搜真正的点号，必须加反斜杠。
  - **`grep -E "^[0-9]{3}" app.log`**：开启扩展正则 (等同于 `egrep`)。
  - **`grep -P "\d+" app.log`**：开启 **Perl 正则** (支持 `\d`, `\w` 等高级语法)。
  - **💡 效率组合：上下文搜索**
    - `grep -C 5 "Error" app.log`：显示包含 Error 的行，及其**上下各 5 行** (Context)。
    - `grep -B 3 "Error" app.log`：显示**前** 3 行 (Before)。
    - `grep -A 3 "Error" app.log`：显示**后** 3 行 (After)。

---

## 3. cut - 快速提取列 (轻量版 awk)
当你只需要按固定分隔符提取列时，`cut` 比 `awk` 快得多。
- `cut -d ':' -f 1 /etc/passwd`
  - `-d ':'`：指定冒号为分隔符。
  - `-f 1`：提取第 1 列。

---

## 4. sort / uniq - 排序与去重
在处理日志或列表时，这两个命令通常是“连体婴儿”。

- **`sort`**：
  - `sort -n`：按**数字大小**排序 (如果不加 -n，10 会排在 2 前面)。
  - `sort -r`：逆序排序。
  - `sort -k 2`：按第 2 列排序。
- **`uniq` (Unique)**：
  - **⚠ 注意**：`uniq` 只能去除**相邻的**重复行，所以使用前必须先 `sort`。
  - `uniq -c`：统计每行出现的次数。
  - `uniq -d`：只显示重复过的行。

---

## 5. tr / column - 字符转换与格式化
- **`tr` (Translate)**：
  - `cat file | tr 'a-z' 'A-Z'`：将所有小写字母转为大写。
  - `cat file | tr -d ' '`：删除所有空格。
- **`column -t`**：让乱糟糟的输出对齐成漂亮的表格。
  - *示例*：`cat /etc/passwd | head | column -t -s ':'` (按冒号对齐显示用户信息)。

---

## 6. sed - 文本替换与修改 (Stream Editor)
不要被 `sed` 吓到，你只需要记住**一个万能公式**。

### 💡 万能替换公式
```bash
sed -i 's/旧内容/新内容/g' 文件名
```

*   **`-i`** (in-place)：**直接修改文件**。
    *   **⚠️ MacOS 用户注意**：Mac 自带的 BSD sed 强制要求备份后缀，必须写成 `sed -i '' 's/.../...'`，或者安装 `gnu-sed`。
*   **`s`** (substitute)：表示“替换”操作。
*   **`g`** (global)：**全局替换**。如果不加 `g`，它只会替换每一行中找到的*第一个*旧内容。

#### 💡 进阶：分隔符冲突与捕获组
1.  **分隔符冲突**：如果内容包含 `/`，可以用 `#` 代替。
    ```bash
    sed -i 's#/var/www#/home/www#g' config.conf
    ```
2.  **捕获组 (Backreferences)**：保留一部分内容，重组另一部分。
    *   **场景**：把 `Name: Alice` 变成 `User: Alice`。
    *   **命令**：`sed -E 's/Name: (.*)/User: \1/'`
    *   **解析**：`()` 包裹的内容会被存入 `\1`, `\2`... 中，在替换部分可以直接引用。

3.  **删除特定行 (`d` 模式)**：
    ```bash
    # 删除第 5 行
    sed -i '5d' file.txt
    
    # 删除所有包含 "password" 的行
    sed -i '/password/d' file.txt
    
    # 删除所有空行 (配合正则)
    sed -i '/^$/d' file.txt
    ```

---

## 7. awk - 数据提取 (列处理)
`sed` 擅长修改行，`awk` 擅长提取列。

- **核心逻辑**：
  - 默认按**空格或Tab**切分。`$1` 是第一列，`$0` 是整行。
  - **指定分隔符 (`-F`)**：`awk -F: '{print $1}' /etc/passwd` (按冒号切分)。
- **实战案例**：
  - **提取第三列（示例）**：`ls -l | awk '{print $3}'` (在 `ls -l` 输出中，第 3 列通常是所有者)。
  - **最后一列**：`awk '{print $NF}'` (不管有多少列，只打印最后一列)。
  - **自定义输出格式 (`OFS`)**：
    `awk -F: 'BEGIN{OFS=" | "} {print $1, $3}' /etc/passwd`
    (输出结果会变成：`root | 0`)
- **内置变量**：
  - **`NR`**：当前行号。
  - **`NF`**：当前行的总列数。
  - **`FS`**：输入字段分隔符 (等同于 `-F`)。
  - **`OFS`**：输出字段分隔符。
- **💡 结构化处理：BEGIN/END**
  - **场景**：计算文件总行数（比 `wc -l` 更灵活）。
    ```bash
    awk 'END {print "Total lines:", NR}' file.txt
    ```
  - **场景**：求和。
    ```bash
    awk '{sum+=$2} END {print "Sum:", sum}' data.txt
    ```

---

## 8. jq - JSON 数据处理 (现代开发必备)
在微服务和 API 时代，`jq` 是处理 JSON 的“瑞士军刀”。

- **基础与格式化**：
  - `cat data.json | jq .`：漂亮地格式化输出。
  - `jq '.name' data.json`：提取特定字段。
- **数组与过滤**：
  - `jq '.users[0]'`：访问数组项。
  - `jq '.[] | select(.age > 25)'`：**条件过滤**，筛选 age 大于 25 的对象。
- **高级变换**：
  - `jq -r '.id'`：**Raw 输出**，去掉引号（方便 shell 脚本后续处理）。
  - `jq '[.items[].name]'`：将所有 name 提取并重构成一个新的数组。
- **场景**：`curl -s https://api.github.com/repos/stedolan/jq | jq '.stargazers_count'`。

---

## 9. xargs - 管道传递增强 (连接器)
`xargs` 的作用是把上一个命令的**输出内容**，转化为下一个命令的**参数**。

- **为什么需要它？**：
  很多命令（如 `rm`, `kill`, `cp`）不接受管道直接输入的内容，它们需要具体的参数。
- **💡 进阶：占位符 `-I` 与并行 `-P`**
  - **占位符**：如果你想把参数放在命令的**中间**，就需要用到 `-I`。
    - `ls *.jpg | xargs -I {} mv {} backup_{}` (将所有 jpg 重命名)。
  - **并行处理 (-P)**：**提速神器**。
    - `find . -name "*.jpg" | xargs -P 4 -I {} convert {} {}.png`
    - **效果**：同时启动 4 个进程进行图片转换，充分利用多核 CPU。
- **典型用法**：
  - `cat server_list.txt | xargs -n 1 ping -c 1`：批量 ping 列表（`-n 1` 表示每次传递 1 个参数）。
  - `find . -name "*.log" -print0 | xargs -0 grep "Error"`：安全处理包含空格的文件名。
- **💡 杀掉特定进程（谨慎）**：
  `ps -ef | grep "[p]ython" | awk '{print $2}' | xargs kill -15`

---

## 10. 终极实战：流水线组合拳
Linux 的威力在于通过管道符 `|` 将多个简单工具组合。

### 💡 必备 Awk / Shell 单行命令 (One-Liners)
| 需求 | 命令 |
| :--- | :--- |
| **求和** | `awk '{sum+=$1} END {print sum}' data.txt` |
| **求平均值** | `awk '{sum+=$1} END {print sum/NR}' data.txt` |
| **打印偶数行** | `awk 'NR%2==0' file.txt` |
| **过滤多列** | `awk '$2 > 10 && $3 == "OK"' log.txt` |

### 场景：统计日志中访问量最大的前 10 个 IP
假设你有一个几百 MB 的 `access.log`，你想知道谁在疯狂请求你的服务器。
```bash
cat access.log | awk '{print $1}' | sort | uniq -c | sort -rn | head -n 10
```
**步骤拆解：**
1.  **`awk '{print $1}'`**：只提取第一列（IP 地址）。
2.  **`sort`**：把相同的 IP 排在一起。
3.  **`uniq -c`**：去重并统计每个 IP 出现的次数（输出格式如：`15 192.168.1.1`）。
4.  **`sort -rn`**：按数字 (`n`) 逆序 (`r`) 再次排序，把次数最多的放最前面。
5.  **`head -n 10`**：只取前 10 行。

### 总结
- **`sed`**：适合**批量修改**、删除文件内容。
- **`awk`**：适合**分析格式化日志**、提取特定列。
- **`xargs`**：适合**把前面的结果变成参数**发给后面的命令（如 `find ... | xargs rm -f`）。
